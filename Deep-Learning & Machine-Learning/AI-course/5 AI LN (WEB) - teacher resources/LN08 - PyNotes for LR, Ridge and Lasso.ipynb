{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Regression models \n",
    "#### We will go over linear regression, regularization, ridge regression and lasso regression. \n",
    "##### (reduced ver of W2 of App ML)\n",
    "Visit my web site for very good sources of ML\n",
    "I use anaconda which has Jupyter in it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression #function used for generating synthetic datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#anaconda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set 1: A synthetic data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only for illustrative purposes. \n",
    "# This one generates random data to make some regression analysis. \n",
    "# We will make regression with other data sets soon. \n",
    "\n",
    "# synthetic dataset for simple regression\n",
    "from sklearn.datasets import make_regression\n",
    "plt.figure()\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n",
    "                            n_informative=1, bias = 150.0,\n",
    "                            noise = 30, random_state=0)\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dimension==>\", X_R1.ndim,\"length==>\", X_R1.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into train set and test set. \n",
    "# the default value i 75% train and 25% test\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X_R1, y_R1,random_state=1)\n",
    "# compare without random_state\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape, X_test.ndim) # gives the size of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainReg = LinearRegression().fit(X_train, y_train)\n",
    "FullReg = LinearRegression().fit(X_R1, y_R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('linear model coeff (Theta_0): {}'.format(TrainReg.coef_))\n",
    "print('linear model intercept (Theta_1): {:.3f}'.format(TrainReg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'.format(TrainReg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'.format(TrainReg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.plot(X_R1, TrainReg.coef_ * X_R1 + TrainReg.intercept_, 'g-')\n",
    "plt.scatter(X_R1, y_R1, marker= 'x', s=50, alpha=0.8)\n",
    "plt.plot(X_R1, FullReg.coef_ * X_R1 + FullReg.intercept_, 'r-')\n",
    "\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set 2: House Pricing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### An example for the house price data. \n",
    "\n",
    "house_price = pd.DataFrame (np.genfromtxt(\"house_price_data_2000.csv\", \\\n",
    "            delimiter=\";\", skip_header=1, dtype=None))\n",
    "house_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(house_price.ndim , house_price.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_house = house_price.iloc[:  , 0:3] #from 0(inclusive) to 3(exclusive)\n",
    "y_house = house_price.iloc[:, -1]\n",
    "\n",
    "#X_house = house_price[:, 0:6]\n",
    "#y_house = house_price[:, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_house.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_house.iloc[:,0], y_house, marker= 'o', s=50)\n",
    "#size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_house.iloc[:,1], y_house, marker= 'o', s=50)\n",
    "# room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_house.iloc[:,2], y_house, marker= 'o', s=50)\n",
    "#bath room for 2000\n",
    "#living room for 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(X_house[:,3], y_house, marker= 'o', s=50)\n",
    "# bathroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plt.scatter(X_house[:,4], y_house, marker= 'o', s=50)\n",
    "# floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plt.scatter(X_house[:,5], y_house, marker= 'x', s=50)\n",
    "# age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_house, y_house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression().fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.score(X_test,y_test) \n",
    "#As the score, it gives R2 values (from stats course)\n",
    "#it does not give the  J(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('linear model intercept (Theta_0): {:.1f}'.format(linreg.intercept_))\n",
    "print('linear model coeff (Theta_1): {}'.format(linreg.coef_))\n",
    "print('R2-Train',round(linreg.score(X_train, y_train),3), \\\n",
    "       'R2-Test',round(linreg.score(X_test, y_test),3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_house.iloc[:,0], y_house, marker= 'o', s=50, alpha=0.8)\n",
    "plt.plot(X_house.iloc[:,0], linreg.coef_[0] * X_house.iloc[:,0] + linreg.intercept_, 'r-')\n",
    "\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression (linear regression with regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use  the same dataset (house pricing)\n",
    "##### We will learn n-fold cross validation and scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#linridge = Ridge(alpha=9000000000).fit(X_train, y_train)\n",
    "\n",
    " linridge = Ridge(alpha=0).fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ridge regression linear model intercept: {}'.format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'.format(linridge.coef_))\n",
    "print('R2-Train',round(linridge.score(X_train, y_train),3), 'R2-Test',round(linridge.score(X_test, y_test),3))\n",
    "\n",
    "print('Number of non-zero features: {}'.format(np.sum(linridge.coef_ != 0)))\n",
    "#we print the coefficients that are non zero. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if  alpha value is 10^9!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-fold Cross Validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "linridge = Ridge(alpha=0) #create a Ridge object \n",
    "#remember the ridge regression is something like this:\n",
    "#linridge = Ridge(alpha=9000000).fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(linridge, X_house, y_house, cv=5, scoring=\"r2\")\n",
    "\n",
    "#put the Ridge object [linridge] to the cross_val_Score \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Why do we have so bad values?????\n",
    "# Because it is an ordered set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-fold Cross Validation with Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "scores = cross_val_score(linridge, X_house, y_house, cv=kf5, scoring=\"r2\")\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to apply a scaler? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# crime data set\n",
    "\n",
    "##X_train, X_test, y_train, y_test = train_test_split(X_house, y_house)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train) # we use X_train to adjust/fit the scaler\n",
    "\n",
    "X_test_scaled  = scaler.transform(X_test) #use the same fit found above to transform X_test\n",
    "\n",
    "linridge = Ridge(alpha=2).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('R2-Train',round(linridge.score(X_train_scaled, y_train),3), \\\n",
    "       'R2-Test',round(linridge.score(X_test_scaled, y_test),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Better in understanding the regularization. \n",
    " ## Recall we used 10^9 to make all of the parameters zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set 3: Another House Pricing from the web. [try at home]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "housing = read_csv(url, header=None)\n",
    "# summarize shape\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = housing.values\n",
    "X_housing, y_housing = data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_housing, y_housing)\n",
    "\n",
    "linridge = Ridge(alpha=0).fit(X_train, y_train)\n",
    "\n",
    "print('R2-Train',round(linridge.score(X_train, y_train),3), 'R2-Test',round(linridge.score(X_test, y_test),3))\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set 4: Customized Computer Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is a dataset which represent the cost of customized computers. \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "CostData1 = pd.read_excel('CostV1.xlsx')\n",
    "CostData1.head()\n",
    "\n",
    "print(\"Dim-->\", CostData1.ndim, \"Length->\",CostData1.shape)\n",
    "\n",
    "CostData1[\"AG-2\"] = CostData1[\"AG-2\"].fillna(0) #fill the NaNs with Zeros\n",
    "CostData1.head()\n",
    "\n",
    "CostData1 = pd.get_dummies(CostData1) #Cha\n",
    "CostData1\n",
    "\n",
    "y_CostData1 = CostData1['Cost']\n",
    "y_CostData1.head()\n",
    "\n",
    "\n",
    "X_CostData1 = CostData1.drop(\"Cost\",axis=1)\n",
    "X_CostData1.head()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_CostData1, y_CostData1,random_state=5)\n",
    "linridge = Ridge(alpha=0).fit(X_train, y_train)                                                   \n",
    "\n",
    "print('Training',round(linridge.score(X_train, y_train),3), 'Test',round(linridge.score(X_test, y_test),3))\n",
    "\n",
    "#for CostV1.xlsx check for random state 1 and 2 and 5\n",
    "#random state 12 is fine with alpha for dataset 2\n",
    "\n",
    "linridge = Ridge(alpha=1).fit(X_train, y_train)                                                   \n",
    "print('Training',round(linridge.score(X_train, y_train),3), 'Test',round(linridge.score(X_test, y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "house_price = pd.DataFrame (np.genfromtxt(\"house_price_data_2000.csv\", delimiter=\";\", skip_header=1, dtype=None))\n",
    "\n",
    "X_house = house_price.iloc[:  , 0:3] #from 0(inclusive) to 3(exclusive)\n",
    "y_house = house_price.iloc[:, 3]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_house, y_house)\n",
    "lassores = Lasso(alpha=10).fit(X_train, y_train)\n",
    "\n",
    "print('Intercept', lassores.intercept_)\n",
    "print('Coeff',lassores.coef_)\n",
    "print('R2-Train',round(lassores.score(X_train, y_train),3), 'R2-Test',round(lassores.score(X_test, y_test),3))\n",
    "\n",
    "print('Number of non-zero features: {}'.format(np.sum(lassores.coef_ != 0)))\n",
    "\n",
    "\n",
    "################################################\n",
    "#### Check for different alpha values. #########\n",
    "################################################\n",
    "\n",
    "for this_alpha in [0, 1, 2, 3, 4, 1000, 10000,100000, 1000000, 10000000, 10000000, 100000000]:\n",
    "\n",
    "#[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40]:\n",
    "    lassores = Lasso(alpha=this_alpha).fit(X_train, y_train)\n",
    "    r2_train = lassores.score(X_train, y_train)\n",
    "    r2_test  = lassores.score(X_test,  y_test)\n",
    "    num_coeff_bigger = np.sum(abs(lassores.coef_) > 0.01)\n",
    "    print('Alpha = {:.3f}\\nnum abs(coeff) > 0.01: {}, r-squared training: {:.3f}, r-squared test: {:.3f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))\n",
    "    print(lassores.coef_)\n",
    "    \n",
    "    \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "# # # Check for different alpha values with normalization       # # # # # # # # # # # #\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train) # we use X_train to adjust/fit the scaler\n",
    "X_test_scaled  = scaler.transform(X_test) #use the same fit found above to transform X_test\n",
    "\n",
    "for this_alpha in [1000, 2000,3000,4000,5000,6000,7000,8000,9000,10000]:\n",
    "\n",
    "#[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40]:\n",
    "    lassores = Lasso(alpha=this_alpha).fit(X_train_scaled, y_train)\n",
    "\n",
    "    r2_train = lassores.score(X_train_scaled, y_train)\n",
    "    r2_test  = lassores.score(X_test_scaled,  y_test)\n",
    "    num_coeff_bigger = np.sum(abs(lassores.coef_) > 1)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 0.01: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In order to apply this, I generate a sythetic data set:7\n",
    "\n",
    "# synthetic dataset for more complex regression\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression #function used for generating synthetic datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Complex regression problem with one input variable')\n",
    "X_F1, y_F1 = make_friedman1(n_samples = 100,\n",
    "                           n_features = 7, random_state=0)\n",
    "\n",
    "plt.scatter(X_F1[:, 2], y_F1, marker= 'o', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1, y_F1, random_state = 0)\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nNow we transform the original input data to add\\n\\\n",
    "polynomial features up to degree 2 (quadratic)\\n')\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "#First try it on a simple dataset\n",
    "\n",
    "X_my = np.random.randint(10, size=(5, 2)) \n",
    "print(\"X_my\", X_my)\n",
    "\n",
    "X_F1_poly_try = poly.fit_transform(X_my) #using the avlues in X_my, generate new set of features \n",
    "#called X_F1-poly\n",
    "\n",
    "print(\"X_F1_poly_try\", X_F1_poly_try)\n",
    "\n",
    "X_F1_poly = poly.fit_transform(X_F1) #using the values in X_F1, generate new set of features called X_F1\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1, random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "#print('(poly deg 2) linear model coeff (w):\\n{}'.format(linreg.coef_))\n",
    "#print('(poly deg 2) linear model intercept (b): {:.3f}'.format(linreg.intercept_))\n",
    "print('(poly deg 2) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2) R-squared score (test): {:.3f}\\n'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nAddition of many polynomial features often leads to\\n\\\n",
    "overfitting, so we often use polynomial features in combination\\n\\\n",
    "with regression that has a regularization penalty, like ridge\\n\\\n",
    "regression.\\n')\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = Ridge(alpha=1).fit(X_train, y_train)\n",
    "\n",
    "#print('(poly deg 2) linear model coeff (w):\\n{}'.format(linreg.coef_))\n",
    "#print('(poly deg 2) linear model intercept (b): {:.3f}'.format(linreg.intercept_))\n",
    "print('(poly deg 2 + ridge) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2 + ridge) R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# or try a lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "lassores = Lasso(alpha=0.01).fit(X_train, y_train)\n",
    "\n",
    "##################\n",
    "## try 0.01    ###\n",
    "##################\n",
    "\n",
    "#print('(poly deg 2) linear model coeff (w):\\n{}'.format(linreg.coef_))\n",
    "#print('(poly deg 2) linear model intercept (b): {:.3f}'.format(linreg.intercept_))\n",
    "print('R2-Train',round(lassores.score(X_train, y_train),4), 'R2-Test',round(lassores.score(X_test, y_test),4))\n",
    "\n",
    "print('Number of non-zero features: {}'.format(np.sum(lassores.coef_ != 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set 5: Crime Data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://archive.ics.uci.edu/ml/datasets/communities+and+crime\n",
    "# a dataset which has about 100 features and try to estimate \n",
    "# several different outputs. These outputs are at the last 12 columns\n",
    "# murders, murdPerPop, robberies, robbbPerPop, burglaries, burglPerPop, larcenies, \n",
    "# larcPerPop, autoTheft, autoTheftPerPop, assaults, assaultPerPop\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "CrimeData  = pd.read_csv(\"CrimeDataSetFULL.csv\", delimiter=\";\",header=0) #choose the titles/labels/headers as the first row\n",
    "\n",
    "CrimeData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Crime = CrimeData.iloc[:, 0:101] #predictive features. \n",
    "\n",
    "y_Crime = CrimeData.iloc[:,101] # value to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_Crime, y_Crime)\n",
    "#try other random states. 0, 1, 2, 3, 4, 5\n",
    "linridge = Ridge(alpha=0).fit(X_train, y_train)\n",
    "\n",
    "print('R2-Train',round(linridge.score(X_train, y_train),3), 'R2-Test',round(linridge.score(X_test, y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We apply 5 fold shuffled cross validation for all of the outputs through a for loop\n",
    "#### That is we find the R2 for all of the output through the for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = {}\n",
    "X_Crime = CrimeData.iloc[:, 0:101]\n",
    "linridge = Ridge(alpha=3)\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=True)\n",
    "for i in range(101,113):\n",
    "    y_Crime = CrimeData.iloc[:,i]\n",
    "    tempscores = cross_val_score(linridge, X_Crime, y_Crime, cv=kf5, scoring=\"r2\")\n",
    "    scores[i] = round(tempscores.mean(),3)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply a min-max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# crime data set\n",
    "y_Crime = CrimeData.iloc[:,108] #this one has the smallest test set.It's larcenies or theft) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_Crime, y_Crime)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train) # we use X_train to adjust/fit the scaler\n",
    "X_test_scaled  = scaler.transform(X_test) #use the same fit found above to transform X_test\n",
    "\n",
    "linridge = Ridge(alpha=3).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###print('ridge regression linear model intercept: {}'.format(linridge.intercept_))\n",
    "#print('ridge regression linear model coeff:\\n{}'.format(linridge.coef_))\n",
    "###print('R-squared score (training): {:.3f}'.format(linridge.score(X_train, y_train)))\n",
    "###print('R-squared score (test): {:.3f}'.format(linridge.score(X_test, y_test)))\n",
    "###print('Number of non-zero features: {}'.format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also checking for different regularization values (alpha value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Crime = CrimeData.iloc[:,108] #again the same y vector (larcenies or theft)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_Crime, y_Crime)\n",
    "\n",
    "for this_alpha in [0, 1, 2, 3, 4, 1000, 10000,100000, 1000000, 10000000, 10000000, 100000000]:\n",
    "\n",
    "#[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train, y_train)\n",
    "    r2_train = linridge.score(X_train, y_train)\n",
    "    r2_test  = linridge.score(X_test,  y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 0.01)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 0.01: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also checking for different regularization values for _normalized data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Crime = CrimeData.iloc[:,108] #again the same y vector (larcenies or theft)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_Crime, y_Crime)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train) # we use X_train to adjust/fit the scaler\n",
    "X_test_scaled  = scaler.transform(X_test) #use the same fit found above to transform X_test\n",
    "\n",
    "for this_alpha in [0, 1, 2, 3, 4, 1000, 10000,100000, 1000000, 10000000, 10000000, 100000000]:\n",
    "\n",
    "#[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40]:\n",
    "    linridge = Ridge(alpha=this_alpha).fit(X_train_scaled, y_train)\n",
    "\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test  = linridge.score(X_test_scaled,  y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 0.01)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 0.01: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _You can also try shuffled n fold cross validation over this._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code is prepared up to here. The rest of the code from here on is a draft. #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New DATASETS for Regression and Classification\n",
    "these are just synthetic datasets to observe what we can do using linear regression, logistic regression and regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "\n",
    "\n",
    "# synthetic dataset for classification (binary) \n",
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with two informative features')\n",
    "X_C2, y_C2 = make_classification(n_samples = 100, n_features=2,\n",
    "                                n_redundant=0, n_informative=2,\n",
    "                                n_clusters_per_class=1, flip_y = 0.1,\n",
    "                                class_sep = 0.5, random_state=0)\n",
    "plt.scatter(X_C2[:, 0], X_C2[:, 1], c=y_C2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more difficult synthetic dataset for classification (binary) \n",
    "# with classes that are not linearly separable\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2, centers = 8,\n",
    "                       cluster_std = 1.3, random_state = 4)\n",
    "y_D2 = y_D2 % 2\n",
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with non-linearly separable classes')\n",
    "plt.scatter(X_D2[:,0], X_D2[:,1], c=y_D2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
